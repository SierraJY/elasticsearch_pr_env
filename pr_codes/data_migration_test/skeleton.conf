# Logstash를 사용하여 PostgreSQL에서 데이터를 가져와 Elasticsearch에 저장하는 구성
# 이 스크립트는 Logstash의 JDBC 플러그인을 사용하여 PostgreSQL에서 데이터를 읽고,
# 불필요한 필드를 제거한 후, Elasticsearch로 전송하는 설정입니다.

# 데이터 입력 단계: PostgreSQL에서 데이터를 가져옵니다.
input {
  # JDBC 플러그인 설정
  jdbc {
    # JDBC 드라이버 라이브러리 위치 (PostgreSQL 드라이버 필요)
    jdbc_driver_library => "/usr/share/logstash/ingest_data/postgresql.jar"
    # 사용할 JDBC 드라이버 클래스
    jdbc_driver_class => "org.postgresql.Driver"
    # PostgreSQL 연결 문자열 (컨테이너 환경에서 내부 네트워크를 사용)
    jdbc_connection_string => "jdbc:postgresql://postgres:5432/mydb"
    # 데이터베이스 사용자명 및 비밀번호
    jdbc_user => "myuser"
    jdbc_password => "mypassword"
    # # 데이터 가져오기 주기 (크론 표현식 사용 - 매 1분마다 실행)
    # schedule => "* * * * *"
    # 실행할 SQL 쿼리 (ecommerce_orders 테이블의 모든 데이터 조회)
    statement => "SELECT * FROM ecommerce_orders"
  }
}

# 데이터 변환 단계: 필드명 변경 및 불필요한 필드 제거
filter {
  # Mutate 필터를 사용하여 데이터 변환 수행
  mutate {
    # order_id 필드를 document_id로 변경 (Elasticsearch의 document ID로 사용)
    rename => {
      "order_id" => "document_id"
    }
  }

  # Elasticsearch에 저장 시 불필요한 Logstash 기본 필드 제거
  # Logstash는 기본적으로 데이터를 처리할 때 자동 필드를 붙입니다:
  # @timestamp: Logstash가 이 이벤트를 처리한 시간
  # @version: Logstash 이벤트의 버전 (기본적으로 "1")
  # 이 필드들이 Elasticsearch에 굳이 필요 없으면 제거
  mutate {
    remove_field => ["@timestamp", "@version"]
  }
}

# 데이터 출력 단계: Elasticsearch로 전송합니다.
output {
  # Elasticsearch로 데이터 전송
  elasticsearch {
    # Elasticsearch의 호스트 주소 (기본 포트 9200)
    hosts => ["http://es01:9200"]
    # 저장할 인덱스 이름
    index => "ecommerce_ws"
    # document_id를 설정하여 동일한 ID의 데이터가 덮어쓰이도록 설정
    document_id => "%{document_id}"
  }
  
  # 표준 출력 (stdout)에서 데이터를 확인할 수 있도록 설정
  # 표준 출력 로그 확인
  stdout { codec => rubydebug }
}

# 참고 문서:
# Logstash JDBC Input Plugin: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html
# Logstash Mutate Filter: https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html
# Logstash Elasticsearch Output Plugin: https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html
